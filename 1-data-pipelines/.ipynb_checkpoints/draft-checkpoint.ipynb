{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:05:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName('govtech_challenge').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mba.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>govtech_challenge</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8ba9dcb8b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'govtech_challenge'),\n",
       " ('spark.app.startTime', '1669431910567'),\n",
       " ('spark.app.id', 'local-1669431911555'),\n",
       " ('spark.driver.port', '54685'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.submitTime', '1669431910461'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/tonyngmk/repo/Data-Engineer-Tech-Challenge/1-data-pipelines/spark-warehouse'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', 'mba.home')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Task\n",
    "\n",
    "Application mobile number is 8 digits Applicant is over 18 years old as of 1 Jan 2022 Applicant has a valid email (email ends with @emailprovider.com or @emailprovider.net) You are required to format datasets in the following manner:\n",
    "\n",
    "Split name into first_name and last_name Format birthday field into YYYYMMDD Remove any rows which do not have a name field (treat this as unsuccessful applications) Create a new field named above_18 based on the applicant's birthday Membership IDs for successful applications should be the user's last name, followed by a SHA256 hash of the applicant's birthday, truncated to first 5 digits of hash (i.e <last_name>_<hash(YYYYMMDD)>) You are required to consolidate these datasets and output the successful applications into a folder, which will be picked up by downstream engineers. Unsuccessful applications should be condolidated and dropped into a separate folder.\n",
    "\n",
    "You can use common scheduling solutions such as cron or airflow to implement the scheduling component. Please provide a markdown file as documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['applications_dataset_1.csv', 'applications_dataset_2.csv']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "upstream_dir = \"/Users/tonyngmk/repo/Data-Engineer-Tech-Challenge/1-data-pipelines/data/input\"\n",
    "upstream_csv = [f for f in os.listdir(upstream_dir) if f.endswith(\".csv\")]\n",
    "upstream_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType\n",
    "\n",
    "defined_schema = StructType() \\\n",
    ".add(\"name\", StringType(), True) \\\n",
    ".add(\"email\", StringType(), True) \\\n",
    ".add(\"date_of_birth\", StringType(), True) \\\n",
    ".add(\"mobile_no\", StringType(), True)\n",
    "\n",
    "sdf = spark.read.format(\"csv\").load(upstream_dir, header=True, schema=defined_schema) # read entire directory\n",
    "# problematic if directory is used for multiple input source of different schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- date_of_birth: string (nullable = true)\n",
      " |-- mobile_no: string (nullable = true)\n",
      "\n",
      "Size: 4999*4\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()\n",
    "\n",
    "row = sdf.count()\n",
    "col = len(sdf.columns)\n",
    "print(f\"Size: {row}*{col}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------+-------------+---------+\n",
      "|name           |email                        |date_of_birth|mobile_no|\n",
      "+---------------+-----------------------------+-------------+---------+\n",
      "|Tony Shepherd  |Tony_Shepherd@petersen.com   |07/03/2016   |711447   |\n",
      "|Sherry Gonzalez|Sherry_Gonzalez@caldwell.biz |14-03-1973   |66744895 |\n",
      "|Ashlee Austin  |Ashlee_Austin@melendez.com   |12/09/1992   |6454197  |\n",
      "|David Brown    |David_Brown@jackson-smith.biz|2001-09-22   |69082983 |\n",
      "|Marc Meyer     |Marc_Meyer@chavez.com        |1996/05/17   |9727376  |\n",
      "+---------------+-----------------------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.limit(5).show(truncate=False) # Peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final function\n",
    "def read(upstream_dir):\n",
    "    \"\"\"\n",
    "    read folder that specifically contains data of\n",
    "    membership applications of e-commerce platform\n",
    "    \"\"\"\n",
    "    defined_schema = StructType() \\\n",
    "    .add(\"name\", StringType(), True) \\\n",
    "    .add(\"email\", StringType(), True) \\\n",
    "    .add(\"date_of_birth\", StringType(), True) \\\n",
    "    .add(\"mobile_no\", StringType(), True)\n",
    "\n",
    "    sdf = spark.read.format(\"csv\").load(upstream_dir, header=True, schema=defined_schema)\n",
    "    return sdf\n",
    "\n",
    "sdf = read(\"/Users/tonyngmk/repo/Data-Engineer-Tech-Challenge/1-data-pipelines/data/input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `mobile_no`\n",
    "- 8 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------+-------------+---------+\n",
      "|name           |email                        |date_of_birth|mobile_no|\n",
      "+---------------+-----------------------------+-------------+---------+\n",
      "|Sherry Gonzalez|Sherry_Gonzalez@caldwell.biz |14-03-1973   |66744895 |\n",
      "|David Brown    |David_Brown@jackson-smith.biz|2001-09-22   |69082983 |\n",
      "|Denise Glass   |Denise_Glass@calderon.org    |1982-05-31   |77863943 |\n",
      "|Shane Davis    |Shane_Davis@smith.net        |2006/06/25   |42361783 |\n",
      "|Laura Harding  |Laura_Harding@jackson.com    |2017-12-18   |32048503 |\n",
      "+---------------+-----------------------------+-------------+---------+\n",
      "\n",
      "Remaining: 937/4999\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length, regexp_extract\n",
    "\n",
    "sdf2 = sdf.withColumn(\"mobile_no\", regexp_extract(sdf.mobile_no, r'\\d+', 0).alias('mobile_no'))\n",
    "sdf2 = sdf2.filter(length(sdf.mobile_no) == 8)\n",
    "sdf2.limit(5).show(truncate=False)\n",
    "\n",
    "print(f\"Remaining: {sdf2.count()}/{sdf.count()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `date_of_birth`\n",
    "\n",
    "- over 18 years old as of 1 Jan 2022\n",
    "- In other words, assert DOB < 1 Jan 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------+---------+--------------+\n",
      "|name|email|date_of_birth|mobile_no|date_of_birth2|\n",
      "+----+-----+-------------+---------+--------------+\n",
      "+----+-----+-------------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce, to_date\n",
    "\n",
    "def parse_date(date_col):\n",
    "    \"\"\"\n",
    "    parse all date formats from str to date type\n",
    "    \"\"\"\n",
    "    date_formats = [\"yyyy-MM-dd\",\n",
    "                    \"yyyy MM dd\",\n",
    "                    \"MM/dd/yyyy\",\n",
    "                    \"yyyy/MM/dd\",\n",
    "                    \"dd-MM-yyyy\"]\n",
    "    return coalesce(*[to_date(date_col, format) for format in date_formats])\n",
    "\n",
    "sdf3 = sdf2.withColumn(\"date_of_birth2\", parse_date(sdf.date_of_birth).alias('date_of_birth2'))\n",
    "sdf3.filter(sdf3.date_of_birth2.isNull()).limit(5).show(truncate=False) # check for remaining unparsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------------------+-------------+---------+\n",
      "|name           |email                           |date_of_birth|mobile_no|\n",
      "+---------------+--------------------------------+-------------+---------+\n",
      "|Sherry Gonzalez|Sherry_Gonzalez@caldwell.biz    |1973-03-14   |66744895 |\n",
      "|David Brown    |David_Brown@jackson-smith.biz   |2001-09-22   |69082983 |\n",
      "|Denise Glass   |Denise_Glass@calderon.org       |1982-05-31   |77863943 |\n",
      "|Lori Zimmerman |Lori_Zimmerman@smith-bradley.biz|1998-02-17   |14572007 |\n",
      "|Aaron Wilson   |Aaron_Wilson@burke-benton.net   |1987-05-17   |75053391 |\n",
      "+---------------+--------------------------------+-------------+---------+\n",
      "\n",
      "Remaining: 758/937\n"
     ]
    }
   ],
   "source": [
    "acceptable_date_from = '2022-01-01'\n",
    "\n",
    "sdf3 = sdf2.withColumn(\"date_of_birth\", parse_date(sdf.date_of_birth).alias('date_of_birth'))\n",
    "sdf3 = sdf3.filter(f\"date_of_birth < DATE '{acceptable_date_from}' - interval '18' years\")\n",
    "sdf3.limit(5).show(truncate=False)\n",
    "\n",
    "print(f\"Remaining: {sdf3.count()}/{sdf2.count()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `email`\n",
    "- Applicant has a valid email (email ends with @emailprovider.com or @emailprovider.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------------------+-------------+---------+\n",
      "|name            |email                           |date_of_birth|mobile_no|\n",
      "+----------------+--------------------------------+-------------+---------+\n",
      "|Aaron Wilson    |Aaron_Wilson@burke-benton.net   |1987-05-17   |75053391 |\n",
      "|Samuel Lee      |Samuel_Lee@gardner-rodriguez.net|1974-09-02   |57636831 |\n",
      "|Jennifer Weaver |Jennifer_Weaver@patterson.com   |1978-10-15   |74152151 |\n",
      "|Leonard Hamilton|Leonard_Hamilton@brown.com      |1988-02-29   |76082775 |\n",
      "|Kathy Phillips  |Kathy_Phillips@floyd.com        |1984-12-16   |57829247 |\n",
      "+----------------+--------------------------------+-------------+---------+\n",
      "\n",
      "Remaining: 526/758\n"
     ]
    }
   ],
   "source": [
    "\n",
    "email_regex_pattern = \"^[-_A-Za-z0-9]+@[-_A-Za-z0-9]+\\\\.(?:com|net)$\"\n",
    "sdf4 = sdf3.filter(sdf.email.rlike(email_regex_pattern))\n",
    "sdf4.limit(5).show(truncate=False)\n",
    "\n",
    "print(f\"Remaining: {sdf4.count()}/{sdf3.count()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. `name`\n",
    "\n",
    "- Split name into `first_name` and `last_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------------------+-------------+---------+----------+---------+\n",
      "|name            |email                           |date_of_birth|mobile_no|first_name|last_name|\n",
      "+----------------+--------------------------------+-------------+---------+----------+---------+\n",
      "|Aaron Wilson    |Aaron_Wilson@burke-benton.net   |1987-05-17   |75053391 |Aaron     |Wilson   |\n",
      "|Samuel Lee      |Samuel_Lee@gardner-rodriguez.net|1974-09-02   |57636831 |Samuel    |Lee      |\n",
      "|Jennifer Weaver |Jennifer_Weaver@patterson.com   |1978-10-15   |74152151 |Jennifer  |Weaver   |\n",
      "|Leonard Hamilton|Leonard_Hamilton@brown.com      |1988-02-29   |76082775 |Leonard   |Hamilton |\n",
      "|Kathy Phillips  |Kathy_Phillips@floyd.com        |1984-12-16   |57829247 |Kathy     |Phillips |\n",
      "+----------------+--------------------------------+-------------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length, regexp_extract\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "first_last_name_regex = r'([A-Za-z]+)\\s+([A-Za-z]+)'\n",
    "\n",
    "sdf5 = sdf4\\\n",
    ".withColumn(\"first_name\", regexp_extract(sdf.name, first_last_name_regex, 1).alias('first_name'))\\\n",
    ".withColumn(\"last_name\", regexp_extract(sdf.name, first_last_name_regex, 2).alias('last_name'))\n",
    "# sdf5 = sdf4.withColumn('output', F.expr(r\"regexp_extract_all(name, '([A-Za-z]+)\\s+([A-Za-z]+)', 1)\"))\n",
    "sdf5.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'regexp_extract(name, ([A-Za-z]+)[ ]+([A-Za-z]+), 1) AS last_name'>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_extract(sdf.name, r'([A-Za-z]+)[ ]+([A-Za-z]+)', 1).alias('last_name')\n",
    "\n",
    "\n",
    "# sdf4 = sdf3.filter(sdf.email.rlike(email_regex_pattern))\n",
    "# sdf4.limit(5).show(truncate=False)\n",
    "\n",
    "# print(f\"Remaining: {sdf4.count()}/{sdf3.count()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Regular git commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feat-1.0-pipeline-spark c4ab88a] feat: add parsing of date of birth, filtering for above 18 years old from specified date logic in notebook\n",
      " Committer: Tony Ng <tonyngmk@MBA.home>\n",
      "Your name and email address were configured automatically based\n",
      "on your username and hostname. Please check that they are accurate.\n",
      "You can suppress this message by setting them explicitly. Run the\n",
      "following command and follow the instructions in your editor to edit\n",
      "your configuration file:\n",
      "\n",
      "    git config --global --edit\n",
      "\n",
      "After doing this, you may fix the identity used for this commit with:\n",
      "\n",
      "    git commit --amend --reset-author\n",
      "\n",
      " 2 files changed, 168 insertions(+), 32 deletions(-)\n",
      "Enumerating objects: 9, done.\n",
      "Counting objects: 100% (9/9), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (5/5), done.\n",
      "Writing objects: 100% (5/5), 1.40 KiB | 1.40 MiB/s, done.\n",
      "Total 5 (delta 3), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To github.com:tonyngmk/Data-Engineer-Tech-Challenge.git\n",
      "   c88ef85..c4ab88a  feat-1.0-pipeline-spark -> feat-1.0-pipeline-spark\n"
     ]
    }
   ],
   "source": [
    "! git add .\n",
    "! git commit -m \"feat: add parsing of email and first and last name in notebook\"\n",
    "! git push origin feat-1.0-pipeline-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
