{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:05:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName('govtech_challenge').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mba.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>govtech_challenge</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8ba9dcb8b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'govtech_challenge'),\n",
       " ('spark.app.startTime', '1669431910567'),\n",
       " ('spark.app.id', 'local-1669431911555'),\n",
       " ('spark.driver.port', '54685'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.submitTime', '1669431910461'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/tonyngmk/repo/Data-Engineer-Tech-Challenge/1-data-pipelines/spark-warehouse'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', 'mba.home')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Task\n",
    "\n",
    "Application mobile number is 8 digits Applicant is over 18 years old as of 1 Jan 2022 Applicant has a valid email (email ends with @emailprovider.com or @emailprovider.net) You are required to format datasets in the following manner:\n",
    "\n",
    "Split name into first_name and last_name Format birthday field into YYYYMMDD Remove any rows which do not have a name field (treat this as unsuccessful applications) Create a new field named above_18 based on the applicant's birthday Membership IDs for successful applications should be the user's last name, followed by a SHA256 hash of the applicant's birthday, truncated to first 5 digits of hash (i.e <last_name>_<hash(YYYYMMDD)>) You are required to consolidate these datasets and output the successful applications into a folder, which will be picked up by downstream engineers. Unsuccessful applications should be condolidated and dropped into a separate folder.\n",
    "\n",
    "You can use common scheduling solutions such as cron or airflow to implement the scheduling component. Please provide a markdown file as documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['applications_dataset_1.csv', 'applications_dataset_2.csv']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "upstream_dir = \"/Users/tonyngmk/repo/Data-Engineer-Tech-Challenge/1-data-pipelines/input\"\n",
    "upstream_csv = [f for f in os.listdir(upstream_dir) if f.endswith(\".csv\")]\n",
    "upstream_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType\n",
    "\n",
    "defined_schema = StructType() \\\n",
    ".add(\"name\", StringType(), True) \\\n",
    ".add(\"email\", StringType(), True) \\\n",
    ".add(\"date_of_birth\", StringType(), True) \\\n",
    ".add(\"mobile_no\", StringType(), True)\n",
    "\n",
    "sdf = spark.read.format(\"csv\").load(upstream_dir, header=True, schema=defined_schema, escape='\"') # read entire directory\n",
    "# problematic if directory is used for multiple input source of different schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- date_of_birth: string (nullable = true)\n",
      " |-- mobile_no: string (nullable = true)\n",
      "\n",
      "Size: 4999*4\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()\n",
    "\n",
    "row = sdf.count()\n",
    "col = len(sdf.columns)\n",
    "print(f\"Size: {row}*{col}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------+-------------+---------+\n",
      "|name           |email                        |date_of_birth|mobile_no|\n",
      "+---------------+-----------------------------+-------------+---------+\n",
      "|Tony Shepherd  |Tony_Shepherd@petersen.com   |07/03/2016   |711447   |\n",
      "|Sherry Gonzalez|Sherry_Gonzalez@caldwell.biz |14-03-1973   |66744895 |\n",
      "|Ashlee Austin  |Ashlee_Austin@melendez.com   |12/09/1992   |6454197  |\n",
      "|David Brown    |David_Brown@jackson-smith.biz|2001-09-22   |69082983 |\n",
      "|Marc Meyer     |Marc_Meyer@chavez.com        |1996/05/17   |9727376  |\n",
      "+---------------+-----------------------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.limit(5).show(truncate=False) # Peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final function\n",
    "def read(upstream_dir):\n",
    "    sdf = spark.read.format(\"csv\").load(upstream_dir, header=True)\n",
    "    return sdf\n",
    "\n",
    "sdf = read(\"/Users/tonyngmk/repo/Data-Engineer-Tech-Challenge/1-data-pipelines/input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check mobile number\n",
    "- 8 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------------------+-------------+---------+\n",
      "|name           |email                         |date_of_birth|mobile_no|\n",
      "+---------------+------------------------------+-------------+---------+\n",
      "|Tony Shepherd  |Tony_Shepherd@petersen.com    |07/03/2016   |711447   |\n",
      "|Ashlee Austin  |Ashlee_Austin@melendez.com    |12/09/1992   |6454197  |\n",
      "|Marc Meyer     |Marc_Meyer@chavez.com         |1996/05/17   |9727376  |\n",
      "|Jennifer Vega  |Jennifer_Vega@jenkins.net     |03/07/1971   |6655 1251|\n",
      "|Emily Christian|Emily_Christian@hardy-ball.biz|1993/11/27   |83586    |\n",
      "+---------------+------------------------------+-------------+---------+\n",
      "\n",
      "22/11/26 12:39:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: mobile_no\n",
      "Expected: mobile_no but found: \n",
      "CSV file: file:///Users/tonyngmk/repo/Data-Engineer-Tech-Challenge/README.md\n",
      "Proportion: 4064/5072\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "sdf_2 = sdf.filter(length(sdf.mobile_no) != 8)\n",
    "sdf_2.limit(5).show(truncate=False) # appears to be\n",
    "\n",
    "print(f\"Proportion: {sdf_2.count()}/{sdf.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Regular git commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feat-1.0-pipeline-spark 7663208] feat: add read spark dataframe in notebook\n",
      " Committer: Tony Ng <tonyngmk@MBA.home>\n",
      "Your name and email address were configured automatically based\n",
      "on your username and hostname. Please check that they are accurate.\n",
      "You can suppress this message by setting them explicitly. Run the\n",
      "following command and follow the instructions in your editor to edit\n",
      "your configuration file:\n",
      "\n",
      "    git config --global --edit\n",
      "\n",
      "After doing this, you may fix the identity used for this commit with:\n",
      "\n",
      "    git commit --amend --reset-author\n",
      "\n",
      " 2 files changed, 47 insertions(+), 1 deletion(-)\n"
     ]
    }
   ],
   "source": [
    "! git add .\n",
    "! git commit -m \"feat: add read spark dataframe in notebook\"\n",
    "! git push origin feat-1.0-pipeline-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
