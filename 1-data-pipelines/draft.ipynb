{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:05:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName('govtech_challenge').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mba.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>govtech_challenge</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8ba9dcb8b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'govtech_challenge'),\n",
       " ('spark.app.startTime', '1669431910567'),\n",
       " ('spark.app.id', 'local-1669431911555'),\n",
       " ('spark.driver.port', '54685'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.submitTime', '1669431910461'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/tonyngmk/repo/Data-Engineer-Tech-Challenge/1-data-pipelines/spark-warehouse'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', 'mba.home')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Task\n",
    "\n",
    "Application mobile number is 8 digits Applicant is over 18 years old as of 1 Jan 2022 Applicant has a valid email (email ends with @emailprovider.com or @emailprovider.net) You are required to format datasets in the following manner:\n",
    "\n",
    "Split name into first_name and last_name Format birthday field into YYYYMMDD Remove any rows which do not have a name field (treat this as unsuccessful applications) Create a new field named above_18 based on the applicant's birthday Membership IDs for successful applications should be the user's last name, followed by a SHA256 hash of the applicant's birthday, truncated to first 5 digits of hash (i.e <last_name>_<hash(YYYYMMDD)>) You are required to consolidate these datasets and output the successful applications into a folder, which will be picked up by downstream engineers. Unsuccessful applications should be condolidated and dropped into a separate folder.\n",
    "\n",
    "You can use common scheduling solutions such as cron or airflow to implement the scheduling component. Please provide a markdown file as documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['applications_dataset_1.csv', 'applications_dataset_2.csv']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "upstream_dir = \"/Users/tonyngmk/repo/Data-Engineer-Tech-Challenge/1-data-pipelines/data/input\"\n",
    "upstream_csv = [f for f in os.listdir(upstream_dir) if f.endswith(\".csv\")]\n",
    "upstream_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, ByteType\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "defined_schema = StructType() \\\n",
    ".add(\"name\", StringType(), True) \\\n",
    ".add(\"email\", StringType(), True) \\\n",
    ".add(\"date_of_birth\", StringType(), True) \\\n",
    ".add(\"mobile_no\", StringType(), True) \\\n",
    "\n",
    "sdf = spark.read.format(\"csv\").load(upstream_dir, header=True, schema=defined_schema) # read entire directory\n",
    "# problematic if directory is used for multiple input source of different schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- date_of_birth: string (nullable = true)\n",
      " |-- mobile_no: string (nullable = true)\n",
      "\n",
      "Size: 4999*4\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()\n",
    "\n",
    "row = sdf.count()\n",
    "col = len(sdf.columns)\n",
    "print(f\"Size: {row}*{col}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------+-------------+---------+-------------+\n",
      "|name           |email                        |date_of_birth|mobile_no|is_successful|\n",
      "+---------------+-----------------------------+-------------+---------+-------------+\n",
      "|Tony Shepherd  |Tony_Shepherd@petersen.com   |07/03/2016   |711447   |null         |\n",
      "|Sherry Gonzalez|Sherry_Gonzalez@caldwell.biz |14-03-1973   |66744895 |null         |\n",
      "|Ashlee Austin  |Ashlee_Austin@melendez.com   |12/09/1992   |6454197  |null         |\n",
      "|David Brown    |David_Brown@jackson-smith.biz|2001-09-22   |69082983 |null         |\n",
      "|Marc Meyer     |Marc_Meyer@chavez.com        |1996/05/17   |9727376  |null         |\n",
      "+---------------+-----------------------------+-------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf.withColumn('is_successful', lit(None).cast(ByteType())) # add new col\n",
    "sdf.limit(5).show(truncate=False) # Peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final function\n",
    "def read(upstream_dir):\n",
    "    \"\"\"\n",
    "    read folder that specifically contains data of\n",
    "    membership applications of e-commerce platform\n",
    "    \"\"\"\n",
    "    defined_schema = StructType() \\\n",
    "    .add(\"name\", StringType(), True) \\\n",
    "    .add(\"email\", StringType(), True) \\\n",
    "    .add(\"date_of_birth\", StringType(), True) \\\n",
    "    .add(\"mobile_no\", StringType(), True)\n",
    "\n",
    "    sdf = spark.read.format(\"csv\").load(upstream_dir, header=True, schema=defined_schema)\n",
    "    sdf = sdf.withColumn('is_successful', lit(1).cast(ByteType())) # add new col\n",
    "    return sdf\n",
    "\n",
    "sdf = read(\"/Users/tonyngmk/repo/Data-Engineer-Tech-Challenge/1-data-pipelines/data/input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `mobile_no`\n",
    "- 8 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------+-------------+---------+-------------+\n",
      "|name           |email                        |date_of_birth|mobile_no|is_successful|\n",
      "+---------------+-----------------------------+-------------+---------+-------------+\n",
      "|Tony Shepherd  |Tony_Shepherd@petersen.com   |07/03/2016   |711447   |0            |\n",
      "|Sherry Gonzalez|Sherry_Gonzalez@caldwell.biz |14-03-1973   |66744895 |1            |\n",
      "|Ashlee Austin  |Ashlee_Austin@melendez.com   |12/09/1992   |6454197  |0            |\n",
      "|David Brown    |David_Brown@jackson-smith.biz|2001-09-22   |69082983 |1            |\n",
      "|Marc Meyer     |Marc_Meyer@chavez.com        |1996/05/17   |9727376  |0            |\n",
      "+---------------+-----------------------------+-------------+---------+-------------+\n",
      "\n",
      "Passed: 937/4999\n",
      "Remaining: 937/4999\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length, regexp_extract, when\n",
    "\n",
    "sdf2 = sdf.withColumn(\"mobile_no\", regexp_extract(sdf.mobile_no, r'\\d+', 0).alias('mobile_no'))\n",
    "\n",
    "condition = length(sdf2.mobile_no) == 8\n",
    "sdf2 = sdf2.withColumn(\"condition\", when(condition, 1).otherwise(0))\n",
    "sdf2 = sdf2.withColumn(\"is_successful\", sdf2.is_successful.bitwiseAND(sdf2.condition).alias('is_successful'))\n",
    "sdf2 = sdf2.drop('condition')\n",
    "sdf2.limit(5).show(truncate=False)\n",
    "\n",
    "print(f\"Passed: {sdf2.filter(condition).count()}/{sdf.count()}\")\n",
    "print(f\"Remaining: {sdf2.filter('is_successful=1').count()}/{sdf.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `date_of_birth`\n",
    "\n",
    "- format to yyyyMMdd\n",
    "- over 18 years old as of 1 Jan 2022\n",
    "- In other words, assert DOB < 1 Jan 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------+---------+--------------+\n",
      "|name|email|date_of_birth|mobile_no|date_of_birth2|\n",
      "+----+-----+-------------+---------+--------------+\n",
      "+----+-----+-------------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce, to_date, date_format\n",
    "\n",
    "def parse_date(date_col):\n",
    "    \"\"\"\n",
    "    parse all date formats from str to date type\n",
    "    \"\"\"\n",
    "    date_formats = [\"yyyy-MM-dd\",\n",
    "                    \"yyyy MM dd\",\n",
    "                    \"MM/dd/yyyy\",\n",
    "                    \"yyyy/MM/dd\",\n",
    "                    \"dd-MM-yyyy\"]\n",
    "    return coalesce(*[to_date(date_col, format) for format in date_formats])\n",
    "\n",
    "sdf3 = sdf2.withColumn(\"date_of_birth2\", parse_date(sdf.date_of_birth).alias('date_of_birth2'))\n",
    "sdf3.filter(sdf3.date_of_birth2.isNull()).limit(5).show(truncate=False) # check for remaining unparsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------+-------------+---------+-------------+\n",
      "|name           |email                        |date_of_birth|mobile_no|is_successful|\n",
      "+---------------+-----------------------------+-------------+---------+-------------+\n",
      "|Tony Shepherd  |Tony_Shepherd@petersen.com   |2016-07-03   |711447   |0            |\n",
      "|Sherry Gonzalez|Sherry_Gonzalez@caldwell.biz |1973-03-14   |66744895 |1            |\n",
      "|Ashlee Austin  |Ashlee_Austin@melendez.com   |1992-12-09   |6454197  |0            |\n",
      "|David Brown    |David_Brown@jackson-smith.biz|2001-09-22   |69082983 |1            |\n",
      "|Marc Meyer     |Marc_Meyer@chavez.com        |1996-05-17   |9727376  |0            |\n",
      "+---------------+-----------------------------+-------------+---------+-------------+\n",
      "\n",
      "Passed: 3935/4999\n",
      "Remaining: 758/4999\n"
     ]
    }
   ],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "sdf3 = sdf2.withColumn(\"date_of_birth\", parse_date(sdf.date_of_birth).alias('date_of_birth'))\n",
    "\n",
    "acceptable_date_from = (datetime.date(2022, 1, 1) - relativedelta(years=18)).strftime('%Y-%m-%d')\n",
    "condition = sdf3.date_of_birth < (lit(acceptable_date_from))\n",
    "sdf3 = sdf3.withColumn(\"condition\", when(condition, 1).otherwise(0))\n",
    "sdf3 = sdf3.withColumn(\"is_successful\", sdf3.is_successful.bitwiseAND(sdf3.condition).alias('is_successful'))\n",
    "sdf3 = sdf3.drop('condition')\n",
    "sdf3.limit(5).show(truncate=False)\n",
    "\n",
    "print(f\"Passed: {sdf3.filter(condition).count()}/{sdf.count()}\")\n",
    "print(f\"Remaining: {sdf3.filter('is_successful=1').count()}/{sdf.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------+-------------+---------+-------------+---------+\n",
      "|name           |email                        |date_of_birth|mobile_no|is_successful|condition|\n",
      "+---------------+-----------------------------+-------------+---------+-------------+---------+\n",
      "|Tony Shepherd  |Tony_Shepherd@petersen.com   |null         |711447   |0            |1        |\n",
      "|Sherry Gonzalez|Sherry_Gonzalez@caldwell.biz |null         |66744895 |1            |0        |\n",
      "|Ashlee Austin  |Ashlee_Austin@melendez.com   |null         |6454197  |0            |1        |\n",
      "|David Brown    |David_Brown@jackson-smith.biz|null         |69082983 |1            |0        |\n",
      "|Marc Meyer     |Marc_Meyer@chavez.com        |null         |9727376  |0            |1        |\n",
      "+---------------+-----------------------------+-------------+---------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Date format\n",
    "sdf3 = sdf3.withColumn(\"date_of_birth\", date_format(sdf3.date_of_birth, \"yyyyMMdd\").alias('date_of_birth'))\n",
    "sdf3.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `email`\n",
    "- Applicant has a valid email (email ends with @emailprovider.com or @emailprovider.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------+-------------+---------+-------------+\n",
      "|name           |email                        |date_of_birth|mobile_no|is_successful|\n",
      "+---------------+-----------------------------+-------------+---------+-------------+\n",
      "|Tony Shepherd  |Tony_Shepherd@petersen.com   |null         |711447   |0            |\n",
      "|Sherry Gonzalez|Sherry_Gonzalez@caldwell.biz |null         |66744895 |0            |\n",
      "|Ashlee Austin  |Ashlee_Austin@melendez.com   |null         |6454197  |0            |\n",
      "|David Brown    |David_Brown@jackson-smith.biz|null         |69082983 |0            |\n",
      "|Marc Meyer     |Marc_Meyer@chavez.com        |null         |9727376  |0            |\n",
      "+---------------+-----------------------------+-------------+---------+-------------+\n",
      "\n",
      "Passed: 3523/4999\n",
      "Remaining: 526/4999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sdf4 = sdf3\n",
    "condition = sdf4.email.rlike(email_regex_pattern)\n",
    "sdf4 = sdf4.withColumn(\"condition\", when(condition, 1).otherwise(0))\n",
    "sdf4 = sdf4.withColumn(\"is_successful\", sdf4.is_successful.bitwiseAND(sdf4.condition).alias('is_successful'))\n",
    "sdf4 = sdf4.drop('condition')\n",
    "sdf4.limit(5).show(truncate=False)\n",
    "\n",
    "print(f\"Passed: {sdf3.filter(condition).count()}/{sdf.count()}\")\n",
    "print(f\"Remaining: {sdf4.filter('is_successful=1').count()}/{sdf.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. `name`\n",
    "\n",
    "- Split name into `first_name` and `last_name`\n",
    "- Remove rows that does not have a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(NOT (trim(name) = ))'>"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_last_name_regex = r'([A-Za-z]+)\\s+([A-Za-z]+)'\n",
    "\n",
    "sdf5 = sdf4\\\n",
    ".withColumn(\"first_name\", regexp_extract(sdf4.name, first_last_name_regex, 1).alias('first_name'))\\\n",
    ".withColumn(\"last_name\", regexp_extract(sdf4.name, first_last_name_regex, 2).alias('last_name'))\n",
    "\n",
    "condition = trim(sdf5.name) != ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------+-------------+---------+-------------+----------+---------+\n",
      "|name           |email                        |date_of_birth|mobile_no|is_successful|first_name|last_name|\n",
      "+---------------+-----------------------------+-------------+---------+-------------+----------+---------+\n",
      "|Tony Shepherd  |Tony_Shepherd@petersen.com   |null         |711447   |0            |Tony      |Shepherd |\n",
      "|Sherry Gonzalez|Sherry_Gonzalez@caldwell.biz |null         |66744895 |0            |Sherry    |Gonzalez |\n",
      "|Ashlee Austin  |Ashlee_Austin@melendez.com   |null         |6454197  |0            |Ashlee    |Austin   |\n",
      "|David Brown    |David_Brown@jackson-smith.biz|null         |69082983 |0            |David     |Brown    |\n",
      "|Marc Meyer     |Marc_Meyer@chavez.com        |null         |9727376  |0            |Marc      |Meyer    |\n",
      "+---------------+-----------------------------+-------------+---------+-------------+----------+---------+\n",
      "\n",
      "Passed: 4999/4999\n",
      "Remaining: 526/4999\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length, regexp_extract, trim\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "first_last_name_regex = r'([A-Za-z]+)\\s+([A-Za-z]+)'\n",
    "\n",
    "sdf5 = sdf4\\\n",
    ".withColumn(\"first_name\", regexp_extract(sdf4.name, first_last_name_regex, 1).alias('first_name'))\\\n",
    ".withColumn(\"last_name\", regexp_extract(sdf4.name, first_last_name_regex, 2).alias('last_name'))\n",
    "\n",
    "\n",
    "condition = trim(sdf5.name) != ''\n",
    "sdf5 = sdf5.withColumn(\"condition\", when(condition, 1).otherwise(0))\n",
    "sdf5 = sdf5.withColumn(\"is_successful\", sdf5.is_successful.bitwiseAND(sdf5.condition).alias('is_successful'))\n",
    "sdf5 = sdf5.drop('condition')\n",
    "sdf5.limit(5).show(truncate=False)\n",
    "# sdf5 = sdf5.filter(\"trim(name) != ''\")\n",
    "\n",
    "# sdf5 = sdf4.withColumn('output', F.expr(r\"regexp_extract_all(name, '([A-Za-z]+)\\s+([A-Za-z]+)', 1)\"))\n",
    "\n",
    "print(f\"Passed: {sdf4.filter(condition).count()}/{sdf.count()}\")\n",
    "print(f\"Remaining: {sdf5.filter('is_successful=1').count()}/{sdf.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- email: string (nullable = true)\n",
      " |-- date_of_birth: string (nullable = true)\n",
      " |-- mobile_no: string (nullable = true)\n",
      " |-- is_successful: integer (nullable = false)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf5 = sdf5.drop('name')\n",
    "sdf5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Regular git commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feat-1.0-pipeline-spark 41f8911] feat: add filtering of successful and unsuccessful for mobile_no and date_of_birth to notebook\n",
      " Committer: Tony Ng <tonyngmk@MBA.home>\n",
      "Your name and email address were configured automatically based\n",
      "on your username and hostname. Please check that they are accurate.\n",
      "You can suppress this message by setting them explicitly. Run the\n",
      "following command and follow the instructions in your editor to edit\n",
      "your configuration file:\n",
      "\n",
      "    git config --global --edit\n",
      "\n",
      "After doing this, you may fix the identity used for this commit with:\n",
      "\n",
      "    git commit --amend --reset-author\n",
      "\n",
      " 2 files changed, 270 insertions(+), 114 deletions(-)\n",
      "Enumerating objects: 9, done.\n",
      "Counting objects: 100% (9/9), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (5/5), done.\n",
      "Writing objects: 100% (5/5), 2.15 KiB | 2.15 MiB/s, done.\n",
      "Total 5 (delta 3), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To github.com:tonyngmk/Data-Engineer-Tech-Challenge.git\n",
      "   07b7422..41f8911  feat-1.0-pipeline-spark -> feat-1.0-pipeline-spark\n"
     ]
    }
   ],
   "source": [
    "! git add .\n",
    "! git commit -m \"feat: add filtering of successful and unsuccessful for email and name to notebook\"\n",
    "! git push origin feat-1.0-pipeline-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
